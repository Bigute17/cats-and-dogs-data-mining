{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 18:07:01.539949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new folders for respective classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'dogs-vs-cats/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/blakecurtsinger/Documents/cats-and-dogs-data-mining/work.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/blakecurtsinger/Documents/cats-and-dogs-data-mining/work.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m os\u001b[39m.\u001b[39;49mmkdir(\u001b[39m'\u001b[39;49m\u001b[39mdogs-vs-cats/images\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/blakecurtsinger/Documents/cats-and-dogs-data-mining/work.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m os\u001b[39m.\u001b[39mmkdir(\u001b[39m'\u001b[39m\u001b[39mdogs-vs-cats/images/cat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/blakecurtsinger/Documents/cats-and-dogs-data-mining/work.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m os\u001b[39m.\u001b[39mmkdir(\u001b[39m'\u001b[39m\u001b[39mdogs-vs-cats/images/dog\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'dogs-vs-cats/images'"
     ]
    }
   ],
   "source": [
    "\n",
    "os.mkdir('dogs-vs-cats/images/cat')\n",
    "os.mkdir('dogs-vs-cats/images/dog')\n",
    "\n",
    "folder = 'dogs-vs-cats/images/'\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file.startswith('cat.'):\n",
    "        shutil.move(folder + file, folder + 'cat')\n",
    "    elif file.startswith('dog.'):\n",
    "        shutil.move(folder + file, folder + 'dog')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('dogs-vs-cats/images/cat')\n",
    "for file in random.sample(files,12000):\n",
    "    os.remove('dogs-vs-cats/images/cat/' + file)\n",
    "\n",
    "files = os.listdir('dogs-vs-cats/images/dog')\n",
    "for file in random.sample(files,12000):\n",
    "    os.remove('dogs-vs-cats/images/dog/' + file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set image directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = Path('../cats-and-dogs-data-mining/dogs-vs-cats/images/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create filepath dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = list(image_dir.glob(r'**/*.jpg')) #find all .jpg files within the current folder\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths)) #how we pull labels\n",
    "\n",
    "filepaths = pd.Series(filepaths, name = 'Filepath').astype(str)\n",
    "labels = pd.Series(labels, name = 'Label')\n",
    "\n",
    "image_df = pd.concat([filepaths, labels], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(image_df, train_size = 0.7, shuffle = True, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows us to load a subset of images at a time, train them and recycle the memory so we don't run out\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255, # scale pixel intensity values from 0 - 255 down to 0 - 1\n",
    "    horizontal_flip=True, # Make our model more resilient to horizontally flipped pics\n",
    "    width_shift_range=0.2, # Shift width by 20%\n",
    "    height_shift_range=0.2, # Shift height by 20%\n",
    "    validation_split = 0.2 # Pull train and test images through the same generato\n",
    ")\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 validated image filenames belonging to 2 classes.\n",
      "Found 140 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow the images (specify how the images will be loaded)\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    x_col=\"Filepath\",\n",
    "    y_col=\"Label\",\n",
    "    target_size = (224,224), # Standardize image size\n",
    "    color_mode='rgb', # Our images are colorized\n",
    "    class_mode='binary', # we have 2 classes only\n",
    "    batch_size = 32, # how many images to load at a time\n",
    "    shuffle = True, # Shuffle for training\n",
    "    seed=42, # makes sure the shuffling is always the same way, and always the same subset\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    x_col=\"Filepath\",\n",
    "    y_col=\"Label\",\n",
    "    target_size = (224,224), # Standardize image size\n",
    "    color_mode='rgb', # Our images are colorized\n",
    "    class_mode='binary', # we have 2 classes only\n",
    "    batch_size = 32, # how many images to load at a time\n",
    "    shuffle = True, # Shuffle for training\n",
    "    seed=42, # makes sure the shuffling is always the same way, and always the same subset\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "test_images = train_generator.flow_from_dataframe(\n",
    "    dataframe = test_df,\n",
    "    x_col=\"Filepath\",\n",
    "    y_col=\"Label\",\n",
    "    target_size = (224,224), # Standardize image size\n",
    "    color_mode='rgb', # Our images are colorized\n",
    "    class_mode='binary', # we have 2 classes only\n",
    "    batch_size = 32, # how many images to load at a time\n",
    "    shuffle = False, # False since we are only evaluating, not training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 17:39:37.978350: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6931 - accuracy: 0.4804 - val_loss: 0.6895 - val_accuracy: 0.5071 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 16s 876ms/step - loss: 0.6918 - accuracy: 0.5375 - val_loss: 0.6880 - val_accuracy: 0.5071 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.6875 - accuracy: 0.5268 - val_loss: 0.6819 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 15s 827ms/step - loss: 0.6796 - accuracy: 0.6071 - val_loss: 0.6755 - val_accuracy: 0.5214 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 17s 910ms/step - loss: 0.6731 - accuracy: 0.5768 - val_loss: 0.6551 - val_accuracy: 0.6357 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 15s 842ms/step - loss: 0.6725 - accuracy: 0.5804 - val_loss: 0.6610 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 15s 830ms/step - loss: 0.6656 - accuracy: 0.5911 - val_loss: 0.6556 - val_accuracy: 0.5714 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 15s 823ms/step - loss: 0.6606 - accuracy: 0.6125 - val_loss: 0.6432 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 15s 831ms/step - loss: 0.6567 - accuracy: 0.6143 - val_loss: 0.6424 - val_accuracy: 0.6286 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 15s 828ms/step - loss: 0.6526 - accuracy: 0.6196 - val_loss: 0.6440 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 16s 881ms/step - loss: 0.6514 - accuracy: 0.6232 - val_loss: 0.6359 - val_accuracy: 0.6214 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 15s 851ms/step - loss: 0.6483 - accuracy: 0.6214 - val_loss: 0.6401 - val_accuracy: 0.6214 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 15s 836ms/step - loss: 0.6414 - accuracy: 0.6482 - val_loss: 0.6731 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 16s 862ms/step - loss: 0.6468 - accuracy: 0.6286 - val_loss: 0.6350 - val_accuracy: 0.6214 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 14s 746ms/step - loss: 0.6599 - accuracy: 0.6036 - val_loss: 0.6546 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 19s 993ms/step - loss: 0.6405 - accuracy: 0.6214 - val_loss: 0.6335 - val_accuracy: 0.6571 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 17s 926ms/step - loss: 0.6456 - accuracy: 0.6250 - val_loss: 0.6469 - val_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 15s 828ms/step - loss: 0.6381 - accuracy: 0.6214 - val_loss: 0.6352 - val_accuracy: 0.6214 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 16s 905ms/step - loss: 0.6428 - accuracy: 0.6429 - val_loss: 0.6625 - val_accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 17s 960ms/step - loss: 0.6461 - accuracy: 0.6107 - val_loss: 0.6319 - val_accuracy: 0.6071 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.6250 - accuracy: 0.6518 - val_loss: 0.6514 - val_accuracy: 0.6143 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 15s 847ms/step - loss: 0.6228 - accuracy: 0.6357 - val_loss: 0.6288 - val_accuracy: 0.6429 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 16s 849ms/step - loss: 0.6244 - accuracy: 0.6500 - val_loss: 0.6507 - val_accuracy: 0.6071 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 16s 850ms/step - loss: 0.6283 - accuracy: 0.6518 - val_loss: 0.6425 - val_accuracy: 0.6214 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 15s 832ms/step - loss: 0.6234 - accuracy: 0.6375 - val_loss: 0.6321 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 17s 950ms/step - loss: 0.6239 - accuracy: 0.6500 - val_loss: 0.6323 - val_accuracy: 0.6643 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 15s 837ms/step - loss: 0.6200 - accuracy: 0.6393 - val_loss: 0.6392 - val_accuracy: 0.6071 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(224, 224, 3)) # One for each color channel\n",
    "# 16 filters, kernal size of 3x3. \n",
    "# The convolutional layer will look at the image, slide a window across the image, and the window\n",
    "# weights will multiply by the pixel values, sum them up, and send that to a new 2D feature\n",
    "# We will end up with a new 2D array with the values. \n",
    "# Filters specify how many times we want to do this full pass over the image.\n",
    "# The kernal size represents how big the window is\n",
    "# The whole point of a Convolutional Neural Network is to extract features that \n",
    "# are useful for predicting\n",
    "# If we were to pass each pixel as an individual feature, the model would be too complex and likely\n",
    "# overfit. Also, there is no way to capture the spatial relationship between the data\n",
    "x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "# maxpool also sends a window across the image, and takes a max of 4 pixels.\n",
    "# allows the next convolutional data to reduce the dimensions of the data and keep the most\n",
    "# important pixels (simplified, high level view of each image)\n",
    "# Each time we maxpool, we lose information, but make it easier for the next layer to grasp\n",
    "# high level relationships in the data\n",
    "x = tf.keras.layers.MaxPool2D()(x)\n",
    "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation = 'relu')(x)\n",
    "x = tf.keras.layers.MaxPool2D()(x)\n",
    "# Average over the first 2 dimensions so that we just end up with 32 features. \n",
    "# These features could be anything like pointy ears for cats, or floppy ears for dogs\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "# create the actual classifier, a 2 hidden layer dense NN\n",
    "x = tf.keras.layers.Dense(128, activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(128, activation = 'relu')(x)\n",
    "# outputs is another dense layer with 1 output value and sigmoid activation since\n",
    "# it is a binary classification task\n",
    "# sigmoid gives it the effect of being betweem 0 or 1\n",
    "# so the output is a single prob estimate of the prob that one of the classes is present in the image\n",
    "# In this case, 1 = dog and 0 = cat\n",
    "# So the output is the probability of a dog since that is the positive class\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "# Model compiler with adam optimizer, binary crossentropy loss, and accuracy as the metric\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# 100 epochs with early stopping callback. Early stopping will look at the validation loss so we can\n",
    "# monitor the validation loss, when the val loss has not improved after 5 epochs, it will stop\n",
    "# training and restore the weights from the best epoch\n",
    "# We chose to reduce the learning rate to stabilize model training. Validation loss was fluctuating\n",
    "# a lot previously.\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data = val_images,\n",
    "    epochs = 100,\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor = \"val_loss\",\n",
    "            patience = 5,\n",
    "            restore_best_weights = True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = \"val_loss\",\n",
    "            patience = 3\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
