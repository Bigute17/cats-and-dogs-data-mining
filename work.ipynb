{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 15:05:16.666476: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new folders for respective classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.mkdir('dogs-vs-cats/images/cat')\n",
    "os.mkdir('dogs-vs-cats/images/dog')\n",
    "\n",
    "folder = 'dogs-vs-cats/images/'\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file.startswith('cat.'):\n",
    "        shutil.move(folder + file, folder + 'cat')\n",
    "    elif file.startswith('dog.'):\n",
    "        shutil.move(folder + file, folder + 'dog')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('dogs-vs-cats/images/cat')\n",
    "for file in random.sample(files,12000):\n",
    "    os.remove('dogs-vs-cats/images/cat/' + file)\n",
    "\n",
    "files = os.listdir('dogs-vs-cats/images/dog')\n",
    "for file in random.sample(files,12000):\n",
    "    os.remove('dogs-vs-cats/images/dog/' + file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set image directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = Path('../cats-and-dogs-data-mining/dogs-vs-cats/images/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create filepath dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = list(image_dir.glob(r'**/*.jpg')) #find all .jpg files within the current folder\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths)) #how we pull labels\n",
    "\n",
    "filepaths = pd.Series(filepaths, name = 'Filepath').astype(str)\n",
    "labels = pd.Series(labels, name = 'Label')\n",
    "\n",
    "image_df = pd.concat([filepaths, labels], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(image_df, train_size = 0.7, shuffle = True, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows us to load a subset of images at a time, train them and recycle the memory so we don't run out\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255, # scale pixel intensity values from 0 - 255 down to 0 - 1\n",
    "    horizontal_flip=True, # Make our model more resilient to horizontally flipped pics\n",
    "    width_shift_range=0.2, # Shift width by 20%\n",
    "    height_shift_range=0.2, # Shift height by 20%\n",
    "    validation_split = 0.2 # Pull train and test images through the same generato\n",
    ")\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 validated image filenames belonging to 2 classes.\n",
      "Found 140 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow the images (specify how the images will be loaded)\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    x_col=\"Filepath\",\n",
    "    y_col=\"Label\",\n",
    "    target_size = (224,224), # Standardize image size\n",
    "    color_mode='rgb', # Our images are colorized\n",
    "    class_mode='binary', # we have 2 classes only\n",
    "    batch_size = 32, # how many images to load at a time\n",
    "    shuffle = True, # Shuffle for training\n",
    "    seed=42, # makes sure the shuffling is always the same way, and always the same subset\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    x_col=\"Filepath\",\n",
    "    y_col=\"Label\",\n",
    "    target_size = (224,224), # Standardize image size\n",
    "    color_mode='rgb', # Our images are colorized\n",
    "    class_mode='binary', # we have 2 classes only\n",
    "    batch_size = 32, # how many images to load at a time\n",
    "    shuffle = True, # Shuffle for training\n",
    "    seed=42, # makes sure the shuffling is always the same way, and always the same subset\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "test_images = train_generator.flow_from_dataframe(\n",
    "    dataframe = test_df,\n",
    "    x_col=\"Filepath\",\n",
    "    y_col=\"Label\",\n",
    "    target_size = (224,224), # Standardize image size\n",
    "    color_mode='rgb', # Our images are colorized\n",
    "    class_mode='binary', # we have 2 classes only\n",
    "    batch_size = 32, # how many images to load at a time\n",
    "    shuffle = False, # False since we are only evaluating, not training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 15:06:17.098489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 19s 960ms/step - loss: 0.6934 - accuracy: 0.5036 - val_loss: 0.6905 - val_accuracy: 0.5071 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 16s 878ms/step - loss: 0.6911 - accuracy: 0.5000 - val_loss: 0.6890 - val_accuracy: 0.5071 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.6904 - accuracy: 0.5304 - val_loss: 0.6856 - val_accuracy: 0.5143 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 16s 874ms/step - loss: 0.6867 - accuracy: 0.5179 - val_loss: 0.6801 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 16s 889ms/step - loss: 0.6831 - accuracy: 0.5500 - val_loss: 0.6796 - val_accuracy: 0.5857 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 16s 869ms/step - loss: 0.6763 - accuracy: 0.5696 - val_loss: 0.6751 - val_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 16s 895ms/step - loss: 0.6682 - accuracy: 0.5821 - val_loss: 0.6686 - val_accuracy: 0.5857 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 16s 859ms/step - loss: 0.6759 - accuracy: 0.5625 - val_loss: 0.6868 - val_accuracy: 0.5571 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 16s 871ms/step - loss: 0.6779 - accuracy: 0.5429 - val_loss: 0.6518 - val_accuracy: 0.6357 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 16s 854ms/step - loss: 0.6596 - accuracy: 0.6143 - val_loss: 0.6326 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 16s 859ms/step - loss: 0.6543 - accuracy: 0.6036 - val_loss: 0.6366 - val_accuracy: 0.6571 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 16s 864ms/step - loss: 0.6571 - accuracy: 0.6250 - val_loss: 0.6393 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 16s 875ms/step - loss: 0.6424 - accuracy: 0.6518 - val_loss: 0.6241 - val_accuracy: 0.6214 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 16s 867ms/step - loss: 0.6448 - accuracy: 0.6196 - val_loss: 0.6251 - val_accuracy: 0.6643 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 16s 881ms/step - loss: 0.6339 - accuracy: 0.6571 - val_loss: 0.6365 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 16s 868ms/step - loss: 0.6352 - accuracy: 0.6125 - val_loss: 0.6327 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 16s 903ms/step - loss: 0.6300 - accuracy: 0.6286 - val_loss: 0.6353 - val_accuracy: 0.6071 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6334 - accuracy: 0.6375 - val_loss: 0.6233 - val_accuracy: 0.6643 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 20s 1s/step - loss: 0.6242 - accuracy: 0.6589 - val_loss: 0.6260 - val_accuracy: 0.6143 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 18s 1s/step - loss: 0.6281 - accuracy: 0.6518 - val_loss: 0.6145 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 17s 940ms/step - loss: 0.6213 - accuracy: 0.6589 - val_loss: 0.6319 - val_accuracy: 0.6286 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.6276 - accuracy: 0.6554 - val_loss: 0.6211 - val_accuracy: 0.6214 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6269 - accuracy: 0.6411 - val_loss: 0.6394 - val_accuracy: 0.6214 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.6238 - accuracy: 0.6589 - val_loss: 0.6287 - val_accuracy: 0.6286 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 17s 920ms/step - loss: 0.6237 - accuracy: 0.6518 - val_loss: 0.6227 - val_accuracy: 0.6714 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(224, 224, 3)) # One for each color channel\n",
    "# 16 filters, kernal size of 3x3. \n",
    "# The convolutional layer will look at the image, slide a window across the image, and the window\n",
    "# weights will multiply by the pixel values, sum them up, and send that to a new 2D feature\n",
    "# We will end up with a new 2D array with the values. \n",
    "# Filters specify how many times we want to do this full pass over the image.\n",
    "# The kernal size represents how big the window is\n",
    "# The whole point of a Convolutional Neural Network is to extract features that \n",
    "# are useful for predicting\n",
    "# If we were to pass each pixel as an individual feature, the model would be too complex and likely\n",
    "# overfit. Also, there is no way to capture the spatial relationship between the data\n",
    "x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "# maxpool also sends a window across the image, and takes a max of 4 pixels.\n",
    "# allows the next convolutional data to reduce the dimensions of the data and keep the most\n",
    "# important pixels (simplified, high level view of each image)\n",
    "# Each time we maxpool, we lose information, but make it easier for the next layer to grasp\n",
    "# high level relationships in the data\n",
    "x = tf.keras.layers.MaxPool2D()(x)\n",
    "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation = 'relu')(x)\n",
    "x = tf.keras.layers.MaxPool2D()(x)\n",
    "# Average over the first 2 dimensions so that we just end up with 32 features. \n",
    "# These features could be anything like pointy ears for cats, or floppy ears for dogs\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "# create the actual classifier, a 2 hidden layer dense NN\n",
    "x = tf.keras.layers.Dense(128, activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(128, activation = 'relu')(x)\n",
    "# outputs is another dense layer with 1 output value and sigmoid activation since\n",
    "# it is a binary classification task\n",
    "# sigmoid gives it the effect of being betweem 0 or 1\n",
    "# so the output is a single prob estimate of the prob that one of the classes is present in the image\n",
    "# In this case, 1 = dog and 0 = cat\n",
    "# So the output is the probability of a dog since that is the positive class\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "# Model compiler with adam optimizer, binary crossentropy loss, and accuracy as the metric\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# 100 epochs with early stopping callback. Early stopping will look at the validation loss so we can\n",
    "# monitor the validation loss, when the val loss has not improved after 5 epochs, it will stop\n",
    "# training and restore the weights from the best epoch\n",
    "# We chose to reduce the learning rate to stabilize model training. Validation loss was fluctuating\n",
    "# a lot previously.\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data = val_images,\n",
    "    epochs = 100,\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor = \"val_loss\",\n",
    "            patience = 5,\n",
    "            restore_best_weights = True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = \"val_loss\",\n",
    "            patience = 3\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test Loss: 0.65541\n",
      "Test Accuracy: 63.67\n"
     ]
    }
   ],
   "source": [
    "# Pull our results as a numpy array\n",
    "results = model.evaluate(test_images, verbose = 0) \n",
    "\n",
    "# Display the loss value to 5 decimal places format that with results[0]\n",
    "# The first value returned will be the loss, the second value will be the accuracy.\n",
    "print('    Test Loss: {:.5f}'.format(results[0]))\n",
    "print('Test Accuracy: {:.2f}'.format(results[1] * 100))\n",
    "\n",
    "# About 64% accuracy. Better than randomly guessing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 7s 670ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tb/b14t5svn6qs25f_zj864ykdm0000gn/T/ipykernel_92105/1115919309.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = (model.predict(test_images) >= 0.5).astype(np.int)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "----------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         CAT       0.61      0.58      0.60       149\n",
      "         DOG       0.61      0.63      0.62       151\n",
      "\n",
      "    accuracy                           0.61       300\n",
      "   macro avg       0.61      0.61      0.61       300\n",
      "weighted avg       0.61      0.61      0.61       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model.predict returns the probability estimate for the image being a dog\n",
    "# we want the classification, not estimate\n",
    "# so we can predict on the test_images, and for each value check if it is at least 0.5\n",
    "# If the prob is > 0.5, it will classify as a dog \n",
    "# This returns a boolean array for each test image\n",
    "# setting .astype returns 0 or 1 instead of true/false\n",
    "predictions = (model.predict(test_images) >= 0.5).astype(np.int)\n",
    "\n",
    "# Create confusion matrix.\n",
    "# .labels are the actual values for the images\n",
    "# labels ensures that cats come first, then dogs\n",
    "cm = confusion_matrix(test_images.labels, predictions, labels = [0,1])\n",
    "\n",
    "# Create classification report\n",
    "# Pass the same arguments but add target names\n",
    "clr = classification_report(test_images.labels, predictions, labels = [0,1], target_names = [\"CAT\",\"DOG\"])\n",
    "\n",
    "# Create figure size\n",
    "plt.figure(figsize=(6,6))\n",
    "# Seaborn heatmap to plot the confusion matix\n",
    "# annotations on, fmt = g avoids scientific notation\n",
    "# Minimum value is 0, color map set to blues, color bar set to off\n",
    "sns.heatmap(cm, annot = True, fmt = 'g', vmin = 0, cmap = 'Blues', cbar = False)\n",
    "\n",
    "# Add some tick marks\n",
    "# ticks = spacing of the tick marks\n",
    "# Lables should be in the same order as earlier\n",
    "plt.xticks(ticks = [0.5,1.5], labels = [\"CAT\", \"DOG\"])\n",
    "plt.yticks(ticks = [0.5,1.5], labels = [\"CAT\", \"DOG\"])\n",
    "\n",
    "# Setting axis labels\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"ConfusionMatrix\")\n",
    "\n",
    "# Show that plot and report\n",
    "plt.show()\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "\n",
    "# Out of all the actual cats, 58% were predicted correctly\n",
    "# Out of all the actual dogs, 63% were predicted correctly\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
